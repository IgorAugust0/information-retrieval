{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG0roNs7UaT0"
      },
      "source": [
        "# Aula prática 8 de ORI 2023-1 - Introdução ao Processamento de Língua Natural com NLTK - 19/09/2023\n",
        "\n",
        "Referência: http://www.facom.ufu.br/~wendelmelo/terceiros/tutorial_nltk.pdf\n",
        "\n",
        "Link para a gravação da aula em 2020-2:\n",
        "\n",
        "https://web.microsoftstream.com/video/240f2485-8865-4b29-89d1-95baeee6377a?channelId=32516ce2-5ee6-4787-8484-ce7fe08df48e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR3c8YK-Vsnm"
      },
      "source": [
        "O NLTK é bibioteca para processamento de língua natural em Python.\n",
        "\n",
        "Para Instalar o nltk, algum dos comandos a seguir deve funcionar em sua linha de comando:\n",
        "\n",
        "* pip install -U nltk\n",
        "* pip3 install -U nltk\n",
        "* py -m pip install -U nltk\n",
        "* python -m pip install -U nltk\n",
        "* python3 -m pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vW8Ff09UINL",
        "outputId": "a6378624-b3e5-4969-8f72-194c3646257d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "#para rodar o pip aqui o colab, colocamos uma exclamação na frente do comando\n",
        "!pip install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ_7n_dShAa7"
      },
      "source": [
        "#Usando o NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiDugHRQhCa1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "help(nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiZnbfkhUc6"
      },
      "source": [
        "É possível baixar subpacotes do nltk com a função download. Vamos usar a função download para baixar o subpacote stopwords, que fornece uma lista de stopwords em diferentes linguagens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmWfGTqihTVX",
        "outputId": "872f5366-2bb8-467b-a4f3-aaa5590e7045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "#só precisamos baixar subpacotes uma única vez. Eles estarão disponíveis a partir daqui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Sby3rG1ie8I"
      },
      "outputs": [],
      "source": [
        "#podemos então obter uma lista de stopwords em língua portuguesa\n",
        "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MTTYVP6kDxC"
      },
      "source": [
        "É possível usar o nltk para tokenização de textos. Para isso, usaremos o subpacote punkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWiedBgYkXJa",
        "outputId": "16fea77d-9277-4ee3-82fb-c8d38b74e020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_nyG3SPkeZl",
        "outputId": "7ca591a1-f5a0-4f74-cf53-343be24570a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A',\n",
              " 'humanidade',\n",
              " 'é',\n",
              " 'desumana',\n",
              " '!',\n",
              " 'Mas',\n",
              " ',',\n",
              " 'ainda',\n",
              " '...',\n",
              " 'temos',\n",
              " 'chance',\n",
              " '?']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#tokenizando texto por palavras\n",
        "frase = \"A humanidade é desumana! Mas, ainda... temos chance?\"\n",
        "palavras = nltk.word_tokenize(frase)\n",
        "palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9w3ziZqlCmT",
        "outputId": "befa3645-815a-44e9-f594-d473d0b3fb0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Oi, Tim!', 'Tá Vivo?', 'Claro!']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#podemos tokenizar por sentenças (frases)\n",
        "texto = \"Oi, Tim! Tá Vivo? Claro!\"\n",
        "frases = nltk.sent_tokenize(texto)\n",
        "frases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAKfvFLolc1r"
      },
      "source": [
        "#Extraindo radicais de palavras\n",
        "\n",
        "Usaremos o extratos rslp, que é o extrator de radicais para língua portuguesa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8aWN3uHlcfy",
        "outputId": "c34f0967-c731-4e89-b75f-5b68292a5633"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"rslp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "me9nGPTLl_Cs",
        "outputId": "95294d5c-b356-4f06-f090-2075dd5854eb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'aprendiz'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extrator = nltk.stem.RSLPStemmer()\n",
        "extrator.stem(\"aprendizagem\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-xyhjQFGma3h",
        "outputId": "88083666-e13c-45da-dc0c-0c03d134c31d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'luminos'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extrator.stem(\"luminosidade\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdZGuV2mmuqw"
      },
      "source": [
        "O nltk possui ferramentas para classificação de palavras\n",
        "\n",
        "O nltk possui um corpus (base de dados) em língua portuguesa chamado mac_morpho. mac_morpho traz exemplos de sentenças (frases) classificadas (etiquetadas)\n",
        "\n",
        "Há três tipos de dados classificados\n",
        "\n",
        "* Por palavras (método tagged_words)\n",
        "* Por sentença (método tagged_sents)\n",
        "* Por parágrafo (método tagged_paras)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8GbU61OprzH",
        "outputId": "2e54f43c-6f36-4419-c87a-c441a600867b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/mac_morpho.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"mac_morpho\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLMyn04SpzNR",
        "outputId": "803e9a5b-5c8a-485a-9c7a-14baf9f8ee15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')], [('Programe', 'V'), ('sua', 'PROADJ'), ('viagem', 'N'), ('a', 'PREP|+'), ('a', 'ART'), ('Exposição', 'NPROP'), ('Nacional', 'NPROP'), ('do', 'NPROP'), ('Zebu', 'NPROP'), (',', ','), ('que', 'PRO-KS-REL'), ('começa', 'V'), ('dia', 'N'), ('25', 'N|AP')], ...]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import mac_morpho\n",
        "\n",
        "frases_classificadas = mac_morpho.tagged_sents()\n",
        "print(frases_classificadas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDXMBSVmqU4O",
        "outputId": "a8f3da2d-14d6-4a90-ef2c-2f33881c853e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Jersei', 'N'), ('atinge', 'V'), ('média', 'N'), ('de', 'PREP'), ('Cr$', 'CUR'), ('1,4', 'NUM'), ('milhão', 'N'), ('em', 'PREP|+'), ('a', 'ART'), ('venda', 'N'), ('de', 'PREP|+'), ('a', 'ART'), ('Pinhal', 'NPROP'), ('em', 'PREP'), ('São', 'NPROP'), ('Paulo', 'NPROP')]\n"
          ]
        }
      ],
      "source": [
        "print( frases_classificadas[0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb4D685UrZwe"
      },
      "source": [
        "# Classificação de palavras com NLTK\n",
        "\n",
        "Vamo fazer um exemplo de classificação de palavras usando o classificador (etiquetador) unigram (unigram tagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6NXzvmerZiz",
        "outputId": "051d0805-b027-43ba-be02-8f97a7ae2b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Nos', 'PROPESS'), ('deram', 'V'), ('espelhos', 'N'), (',', ','), ('e', 'KC'), ('vimos', 'V'), ('um', 'ART'), ('mundo', 'N'), ('doente', 'ADJ'), ('!', '!')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "#obtendo as sentenças classificdas do mac_morpho\n",
        "sentencas_classificadas = nltk.corpus.mac_morpho.tagged_sents()\n",
        "\n",
        "#instanciando um etiquetador unigram, e passando as\n",
        "#frases etiquetadas para ele treinar\n",
        "classificador_unigram = nltk.tag.UnigramTagger( sentencas_classificadas )\n",
        "\n",
        "#definindo um texto para ser classificado\n",
        "texto = \"Nos deram espelhos, e vimos um mundo doente!\"\n",
        "tokens_texto = nltk.word_tokenize( texto )  #separa em tokens de palavras\n",
        "\n",
        "classificacao = classificador_unigram.tag( tokens_texto )\n",
        "\n",
        "print(classificacao)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW2hUtGIt0Hh",
        "outputId": "44aed12f-709d-4e47-cf83-b1ee45b07bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Já', 'ADV'), ('chegou', 'V'), ('o', 'ART'), ('disco', 'N'), ('voador', 'ADJ'), ('!', '!')]\n"
          ]
        }
      ],
      "source": [
        "frase2 = \"Já chegou o disco voador!\"\n",
        "tokens_texto2 = nltk.word_tokenize( frase2 )\n",
        "\n",
        "classif2 = classificador_unigram.tag( tokens_texto2 )\n",
        "print(classif2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IgBC7OkuwXp"
      },
      "source": [
        "Pode-se usar os etiquetadores BigramTagger e TrigramTagger para fazer a clasisficação. A diferença entre eles é que unigram observa um token de cada vez, ao passo que bigram observa dois tokens em sequeência (considera pares de tokens), e o trigram 3 (considera trio de palavras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5Bb6iEvRcH"
      },
      "source": [
        "É possível combinar diversos classificados para uso em cadeia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVaIpzkevVpL",
        "outputId": "f2058cb3-f71d-476c-dfa2-ba5a801eb3ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('O', 'ART'), ('rato', 'N'), ('Romário', 'NPROP'), ('roeu', 'N'), ('a', 'PREP'), ('roupa', 'N'), ('do', 'KS'), ('rei', 'N'), ('de', 'PREP'), ('Roma', 'NPROP'), ('na', 'NPROP'), ('rinha', 'N')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "#obtendo as sentenças classificdas do mac_morpho\n",
        "sentencas_classificadas = nltk.corpus.mac_morpho.tagged_sents()\n",
        "\n",
        "#cria um classificador default que classifica tudo como substantivo\n",
        "#se os demais classicadores falharem, a classsificação default de substantivo será usada\n",
        "classificador0 = nltk.DefaultTagger('N')\n",
        "\n",
        "classificador1 = nltk.UnigramTagger( sentencas_classificadas, backoff = classificador0 )\n",
        "classificador2 = nltk.BigramTagger( sentencas_classificadas, backoff=classificador1 )\n",
        "classificador3 = nltk.TrigramTagger( sentencas_classificadas, backoff = classificador2 )\n",
        "\n",
        "texto3 = \"O rato Romário roeu a roupa do rei de Roma na rinha\"\n",
        "tokens_texto3 = texto3.split()   #separa as palavras\n",
        "\n",
        "classificacao3 = classificador3.tag( tokens_texto3 )\n",
        "print(classificacao3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYzlAM9bxrM-"
      },
      "source": [
        "Como o processo de treinar o classificador pode ser muito demorado, pode-se salvar o classificador já treinado em um arquivo para posterior carregamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38RSe2DrySXK"
      },
      "outputs": [],
      "source": [
        "#salvando nosso classificador em um arquivo\n",
        "import pickle\n",
        "\n",
        "arquivo = open( \"classificador.bin\", \"wb\" )\n",
        "pickle.dump( classificador3, arquivo )\n",
        "arquivo.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxiSE5wHzGRC",
        "outputId": "764001ae-af6f-437b-fc82-186c5497c2ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('O', 'ART'), (' ', 'N'), ('s', 'N'), ('o', 'ART'), ('l', 'N'), (' ', 'N'), ('n', 'N'), ('a', 'PREP'), ('s', 'N'), ('c', 'N'), ('e', 'KC'), (' ', 'N'), ('p', 'N'), ('a', 'PREP'), ('r', 'N'), ('a', 'PREP'), (' ', 'N'), ('t', 'N'), ('o', 'ART'), ('d', 'NPROP'), ('o', 'ART'), ('s', 'N'), (',', ','), (' ', 'N'), ('s', 'N'), ('ó', 'IN'), (' ', 'N'), ('n', 'N'), ('ã', 'N'), ('o', 'ART'), (' ', 'N'), ('s', 'N'), ('a', 'PREP'), ('b', 'N'), ('e', 'KC'), (' ', 'N'), ('q', 'N'), ('u', 'N'), ('e', 'KC'), ('m', 'N'), (' ', 'N'), ('n', 'N'), ('ã', 'N'), ('o', 'ART'), (' ', 'N'), ('q', 'N'), ('u', 'N'), ('e', 'KC'), ('r', 'N'), ('!', '!')]\n"
          ]
        }
      ],
      "source": [
        "#carregamento agora o classificador salvo no arquivo\n",
        "import pickle\n",
        "arquivo = open( \"classificador.bin\", \"rb\")\n",
        "classif = pickle.load( arquivo )  # carrega o classificador salvo no arquivo\n",
        "\n",
        "texto = \"O sol nasce para todos, só não sabe quem não quer!\"\n",
        "tokens = nltk.word_tokenize( texto )\n",
        "\n",
        "classificacao = classif.tag( texto )\n",
        "print(classificacao)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
